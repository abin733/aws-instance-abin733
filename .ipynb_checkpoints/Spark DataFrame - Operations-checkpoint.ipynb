{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame - Operations\n",
    "Now that you know the basics, let's get into operations. \n",
    "\n",
    "Objective: This exercise is similar to the Basics exercise, but uses DataFrame methods instead of SQL. We'll also be going through some more complex operations with a more realisitic dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('operations').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas can only be inferred for CSV files. \n",
    "df = spark.read.csv('Datasets/apple_stock_data.csv', inferSchema=True, header=True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a better look at the data.\n",
    "# We know that we can show a DataFrame, but that's resulted in a mess! \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Instead, let's just grab the first row. Much neater! \n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though we know SQL is available, let's try out some of the DataFrame methods.\n",
    "# For this example, let's have a look at the opeening and closing value where close is less than 500.\n",
    "df.filter(\"Close < 500\").select('Open','Close').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use Python within the DataFrame filter method!\n",
    "df.filter(df['Close'] < 500).select('Open','Close').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can use multiple operations! \n",
    "# Here we're looking for significant increases in stock.\n",
    "df.filter( (df['Close'] > 500) & (df['Open'] < 495) ).select('Open','Close').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Collect\n",
    "You may have noticed that showing a DataFrame can be quite messy and useless. Instead, let's try using the collect method to visualise the data. It's not necessarily better, just a different method of achieving similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick a row of data with a low of $197.16 and collect it.  \n",
    "employeeResult = df.filter(df['Low'] == 197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When we collect it, you may notice an interesting format. \n",
    "employeeResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select the first row of data to shed the outer brackets.\n",
    "employeeRow = employeeResult[0]\n",
    "\n",
    "employeeRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then visualise it simply as a dictionary. \n",
    "employeeRow.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why convert it into a dictionary? Because dictionaries have a lot of methods available.\n",
    "# For example, we can simply call volume from the dictionary. \n",
    "employeeRow.asDict()['Volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation and Dates\n",
    "Let's shift gears a bit and focus on something different. Instead of simply eploring the data, let's try to find the average stock closing price per year. To do this, we'll first have to manipulate the Date column. Let's begin! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the relevant functions.\n",
    "from pyspark.sql.functions import dayofmonth,month,hour,year,format_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And create a new column using the year function to manipulate date. \n",
    "df_with_year = df.withColumn(\"Year\",year(df[\"Date\"]))\n",
    "\n",
    "df_with_year.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's sumamrise the data by year, find the mean of each year and select the two columns we'd like to see.\n",
    "df_summary = df_with_year.groupBy(\"Year\").mean().select(['Year','avg(Close)'])\n",
    "df_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the data may be accurate, it's not necessarily appropriate in a professional context. Instead, let's make a few adjustments to make it more appealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make it more visually appealing, let's format the mean to two decimal places.\n",
    "df_formatted = df_summary.select(['Year', format_number(\"avg(Close)\",2)])\n",
    "df_formatted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change the name of the column to something that makes sense.\n",
    "df_renamed = df_formatted.withColumnRenamed(\"format_number(avg(Close), 2)\",\"Average Closing Price\")\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally order it by year.\n",
    "df_renamed.orderBy('Year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! At this stage, it's a good idea to continue exploring the basics of DataFrames. Try different methods or reading the documentation.\n",
    "\n",
    "When you feel comfortable, move on to the DataFrame Data Cleaning Exercise. \n",
    "\n",
    "If you would like a simpler aggregation example, try the DataFrame Aggregation Exercise. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
