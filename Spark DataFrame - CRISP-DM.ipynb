{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame - Basics\n",
    "\n",
    "Let's start off with the fundamentals of Spark DataFrame. \n",
    "\n",
    "Objective: In this exercise, you'll find out how to start a spark session, read in data, explore the data and manipuluate the data (using DataFrame syntax as well as SQL syntax). Let's get started! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Python libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the data. Note that it's in the format of JSON.\n",
    "df = spark.read.json('Datasets/people.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The show method allows you visualise DataFrames. We can see that there are two columns. \n",
    "df.show()\n",
    "\n",
    "# You could also try this. \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the describe method get some general statistics on our data too. Remember to show the DataFrame!\n",
    "# But what about data type?\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For type, we can use print schema. \n",
    "# But wait! What if you want to change the format of the data? Maybe change age to an integer instead of long?\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import in the relevant types.\n",
    "from pyspark.sql.types import (StructField,StringType,IntegerType,StructType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then create a variable with the correct structure.\n",
    "data_schema = [StructField('age',IntegerType(),True),\n",
    "              StructField('name',StringType(),True)]\n",
    "\n",
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we can read in the data using that schema. If we print the schema, we can see that age is now an integer. \n",
    "df = spark.read.json('Datasets/people.json', schema=final_struct)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also select various columns from a DataFrame. \n",
    "df.select('age').show()\n",
    "\n",
    "# We could split up these steps, first assigning the output to a variable, then showing that variable. As you see, the output is the same.\n",
    "ageColumn = df.select('age')\n",
    "\n",
    "ageColumn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also add columns, manipulating the DataFrame.\n",
    "\n",
    "df.withColumn('double_age',df['age']*2).show()\n",
    "\n",
    "# But note that this doesn't alter the original DataFrame. You need to assign the output to a new variable in order to do so.\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can rename columns too! \n",
    "df.withColumnRenamed('age', 'my_new_age').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing SQL\n",
    "We can query a DataFrame as if it were a table! Let's see a few examples of that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we have to register the DataFrame as a SQL temporary view.\n",
    "df.createOrReplaceTempView('people')\n",
    "\n",
    "# After that, we can use the SQL programming language for queries. \n",
    "results = spark.sql(\"SELECT * FROM people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's another example:\n",
    "results = spark.sql(\"SELECT age FROM people WHERE age >= 19\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert the DataFrame to a Pandas DataFrame for plotting\n",
    "df_pandas = df.toPandas()\n",
    "\n",
    "# Plotting using matplotlib\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df_pandas['age'], bins=20, color='skyblue')\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "file_names = [\n",
    "    \"Datasets/2022 Policy Strength.csv\",\n",
    "    \"Datasets/2022 Ocean Science Funding.csv\",\n",
    "    \"Datasets/2022 Marine Key Biodiversities Areas Percentage.csv\",\n",
    "    \"Datasets/2018 Sustainable Fish Stock.csv\",\n",
    "    \"Datasets/1969 - 2018 capture-fisheries-vs-aquaculture.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file_name in file_names:\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Data\n",
    "for df in dfs:\n",
    "    df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization for each table\n",
    "# 1. 2022 Policy Strength\n",
    "df_0 = dfs[0]\n",
    "df_0 = df_0.withColumn(\"Value\", df_0[\"Value\"].cast(IntegerType()))\n",
    "df_0 = df_0.na.drop()\n",
    "df_0_pd = df_0.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting with Pandas (alternative to Matplotlib for this step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_0_pd.plot(kind=\"bar\", x=\"GeoAreaName\", y=\"Value\", rot=90)\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Policy Strength\")\n",
    "plt.title(\"2022 Policy Strength\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 2022 Ocean Science Funding\n",
    "df_1 = dfs[1]\n",
    "df_1 = df_1.withColumn(\"GDP\", df_1[\"GDP\"].cast(IntegerType()))\n",
    "df_1 = df_1.na.drop()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "df_1_pd = df_1.toPandas()\n",
    "\n",
    "# Plot using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_1_pd[\"GeoAreaName\"], df_1_pd[\"GDP\"])\n",
    "plt.xlabel(\"GDP Allocation\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.title(\"2022 Ocean Science Funding\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 2022 Marine Key Biodiversities Areas Percentage (scatter plot)\n",
    "df_2 = dfs[2]\n",
    "df_2 = df_2.withColumn(\"Percent\", df_2[\"MPA Percent\"].cast(IntegerType()))\n",
    "df_2 = df_2.na.drop()\n",
    "df_2_pd = df_2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with Pandas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_2_pd[\"GeoAreaName\"], df_2_pd[\"Percent\"])\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"2022 Marine Key Biodiversities Areas Percentage\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 2018 Sustainable Fish Stock\n",
    "df_3 = dfs[3]\n",
    "df_3 = df_3.withColumn(\"Percentage\", df_3[\"Percentage\"].cast(IntegerType()))\n",
    "df_3 = df_3.na.drop()\n",
    "df_3_pd = df_3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting with Pandas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_3_pd[\"GeoAreaName\"], df_3_pd[\"Percentage\"])\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Sustainability Rating\")\n",
    "plt.title(\"2018 Sustainable Fish Stock\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 1969-2018 Capture Fisheries vs Aquaculture\n",
    "df_4 = dfs[4]\n",
    "df_4 = df_4.na.drop()\n",
    "df_4 = df_4.withColumn(\"Year\", df_4[\"Year\"].cast(IntegerType()))\n",
    "df_4 = df_4.withColumn(\"Capture\", df_4[\"Capture\"].cast(IntegerType()))\n",
    "df_4 = df_4.withColumn(\"Aquaculture\", df_4[\"Aquaculture\"].cast(IntegerType()))\n",
    "df_4 = df_4.na.drop()\n",
    "df_4_pd = df_4.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_4_pd[\"Year\"], df_4_pd[\"Capture\"], color=\"red\", label=\"Capture Fisheries\")\n",
    "plt.plot(df_4_pd[\"Year\"], df_4_pd[\"Aquaculture\"], color=\"blue\", label=\"Aquaculture\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Tonnes\")\n",
    "plt.legend()\n",
    "plt.title(\"Capture Fisheries vs Aquaculture (1969-2018)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality report for each dataframe\n",
    "for i, df in enumerate(dfs):\n",
    "    print(f\"Table {i + 1}:\")\n",
    "    df.printSchema()\n",
    "    df.describe().show()\n",
    "\n",
    "# Data Preparation\n",
    "# In this section, you can add data preprocessing and transformation steps as needed.\n",
    "# For example, renaming columns, merging dataframes, handling missing values, and feature engineering.\n",
    "\n",
    "# Model Building\n",
    "# You can add machine learning model building and evaluation steps in this section.\n",
    "# For example, building a classification or regression model to address the overfishing problem.\n",
    "\n",
    "# Action Plan and Implementation\n",
    "# Outline your plan for implementing the solution, monitoring, and continuous improvement.\n",
    "\n",
    "# 07-DM\n",
    "# Execute DM task (if applicable)\n",
    "\n",
    "# 08-INT\n",
    "# Summarize Results\n",
    "\n",
    "# Add relevant tables or graphs\n",
    "\n",
    "# 09-ACT\n",
    "# Describe the Action Plan for Implementation, Observation, and Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# 03-DP\n",
    "\n",
    "# Print the current dataframe\n",
    "dfs[3].select(\"GeoAreaName\", \"Percentage\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add SDG14_4_1 column\n",
    "dfs[3] = dfs[3].withColumn(\"SDG14_4_1\", when(dfs[3][\"Percentage\"] > 50, 1).otherwise(0))\n",
    "\n",
    "# Print the updated dataframe\n",
    "dfs[3].select(\"GeoAreaName\", \"Percentage\", \"SDG14_4_1\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# Rename \"Entity\" column to \"GeoAreaName\" in table 5\n",
    "dfs[4] = dfs[4].withColumnRenamed(\"Entity\", \"GeoAreaName\")\n",
    "\n",
    "# Merge the tables based on the \"GeoAreaName\" column\n",
    "merged_df = dfs[0]\n",
    "for i in range(1, len(dfs)):\n",
    "    merged_df = merged_df.join(dfs[i], on=[\"GeoAreaName\"], how=\"outer\")\n",
    "\n",
    "# Display the merged table\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "# Count the number of rows\n",
    "num_rows_left = merged_df.count()\n",
    "\n",
    "# Display the dataframe without missing values and the count\n",
    "print(\"Number of rows left:\", num_rows_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04-DT: Data Transformation\n",
    "\n",
    "# Remove the \"Code\" column from the merged_df dataframe\n",
    "merged_df = merged_df.drop(\"Code\")\n",
    "\n",
    "# Transform the \"SDG14_4_1\" column values to 1 if true, otherwise 0\n",
    "from pyspark.sql.functions import expr\n",
    "merged_df = merged_df.withColumn(\"SDG14_4_1\", when(expr(\"SDG14_4_1 = true\"), 1).otherwise(0))\n",
    "\n",
    "# Display the modified merged table (first 5 rows)\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with None values in the \"log_SDG14_4_1\" column\n",
    "filtered_df = merged_df.filter(merged_df[\"SDG14_4_1\"].isNotNull())\n",
    "\n",
    "# Plot a histogram of the log values\n",
    "import matplotlib.pyplot as plt\n",
    "log_values = filtered_df.select(\"SDG14_4_1\").rdd.flatMap(lambda x: x).collect()\n",
    "plt.hist(log_values, bins=[0, 0.5, 1], edgecolor='black')\n",
    "plt.xlabel(\"(SDG14_4_1)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of (SDG14_4_1)\")\n",
    "plt.xticks([0,0.5, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate counts of 0s and 1s in the \"SDG14_4_1\" column\n",
    "counts = merged_df.groupBy(\"SDG14_4_1\").count()\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the modified merged table (first 5 rows)\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define Features (X) and Target (y)\n",
    "features_to_predict = [\"GDP\", \"MPA Percent\", \"Aquaculture\", \"Value\"]\n",
    "assembler = VectorAssembler(inputCols=features_to_predict, outputCol=\"features\")\n",
    "data = assembler.transform(merged_df)\n",
    "data = data.withColumnRenamed(\"SDG14_4_1\", \"label\")\n",
    "\n",
    "# Convert boolean target 'label' to integers (1 for True, 0 for False)\n",
    "data = data.withColumn(\"label\", data[\"label\"].cast(\"int\"))\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(data)\n",
    "data = scaler_model.transform(data)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Build a Linear Regression model\n",
    "lr = LinearRegression(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(metricName=\"mae\")\n",
    "mae = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print the Mean Absolute Error (MAE)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Create a DataFrame for results\n",
    "results = predictions.select(\"GDP\", \"MPA Percent\", \"Aquaculture\", \"Value\", \"label\", \"prediction\")\n",
    "results.show()\n",
    "\n",
    "# Plotting Predictor Importance (requires exporting data for Matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect feature importances data from the model\n",
    "feature_importance_data = lr_model.coefficients\n",
    "\n",
    "# Convert to a list for plotting\n",
    "feature_importance_list = [float(val) for val in feature_importance_data]\n",
    "\n",
    "# List of feature names (adjust as needed)\n",
    "predictor_names = features_to_predict\n",
    "\n",
    "# Plot the feature importance using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(predictor_names, feature_importance_list)\n",
    "plt.xlabel(\"Predictors\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title(\"Predictor Importance in SDG14.4.1 Prediction\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a Summary Table\n",
    "model_name = \"Linear Regression Model\"\n",
    "summary_data = [\n",
    "    (model_name, \"Number of Features\", len(features_to_predict)),\n",
    "    (model_name, \"Number of Epochs\", 0),\n",
    "    (model_name, \"Batch Size\", 0),\n",
    "    (model_name, \"Mean Absolute Error (MAE)\", mae),\n",
    "]\n",
    "\n",
    "# Calculate correct and wrong predictions\n",
    "correct_predictions = predictions.filter(predictions[\"label\"] == predictions[\"prediction\"]).count()\n",
    "total_predictions = predictions.count()\n",
    "wrong_predictions = total_predictions - correct_predictions\n",
    "correct_percentage = (correct_predictions / total_predictions) * 100\n",
    "wrong_percentage = 100 - correct_percentage\n",
    "\n",
    "# Add rows to the summary data\n",
    "summary_data.extend([\n",
    "    (model_name, \"Total\", total_predictions),\n",
    "    (model_name, \"Correct\", correct_predictions),\n",
    "    (model_name, \"Wrong\", wrong_predictions),\n",
    "    (model_name, \"Correct Percentage\", correct_percentage),\n",
    "    (model_name, \"Wrong Percentage\", wrong_percentage),\n",
    "])\n",
    "\n",
    "# Create an RDD from the list of tuples\n",
    "summary_data_rdd = spark.sparkContext.parallelize(summary_data)\n",
    "\n",
    "# Create a PySpark DataFrame for the summary table\n",
    "summary_table = spark.createDataFrame(summary_data_rdd, [\"Model\", \"Metric\", \"Value\"])\n",
    "\n",
    "# Display the summary table\n",
    "summary_table.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done with this tutorial, let's move on to Spark DataFrame Operations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
