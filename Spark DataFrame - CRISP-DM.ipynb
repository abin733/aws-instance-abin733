{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame - CRISP-DM\n",
    "\n",
    "1\tSituation Understanding\n",
    "\n",
    "1.1\tIdentify the objectives of the situation.\n",
    "\n",
    "The analysis that we wish to conduct is critical because the world's oceans are currently facing multiple challenges and, most importantly, the depletion of the world's oceans by 2048 (Worm et al., 2006) due to various reasons such as overfishing, habitat destruction, pollution, and climate change. \n",
    "\n",
    "These challenges pose significant threats to marine biodiversity and the sustainability of ocean resources. By conducting this analysis, we aim to assess the potential solution to overfishing and identify challenges and considerations that need to be addressed for implementing either aquaculture, marine protected areas, or many others as a solution. The success criteria for this analysis include:\n",
    "\n",
    "1.\tAquaculture's potential as a sustainable solution to overfishing is assessed by providing evidence-based findings and recommendations.\n",
    "\n",
    "2.\tEvaluating the effectiveness of marine protected areas as a passive solution to sustainable fish stock health.â€™\n",
    "\n",
    "3.\tI am developing a prediction model with a confidence interval of more than 90% to establish the main factors of SDG14 ratings.\n",
    "\n",
    "4.\tIt produces a comprehensive list of challenges and considerations that must be addressed for implementing aquaculture to solve overfishing.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\tData Understanding\n",
    "\n",
    "The data for this project was collected from publicly available sources, including the UN FAO, UN Statistics, the World Bank - World Development Indicator Database, and SeaAroundUs. It involved catch data (FAO, 2023), aquaculture production data (OECD, 2023), and other relevant environmental and socioeconomic variables.\n",
    "Catch data refers to the recorded amount of fish or other marine organisms caught or harvested from the ocean. This data was often used to monitor the health of fish populations and inform fisheries management practices. The UN Statistics and World Bank - World Development Indicator Database (World Bank, 2023) provided additional data on factors such as gross domestic product (UN Stats, 2023), population, and environmental indicators. SeaAroundUs (2023) offered additional catch data and other metrics related to global fisheries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('basics').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary Python libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import isnan, when, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CSV files\n",
    "file_names = [\n",
    "    \"Datasets/2022 Policy Strength.csv\",\n",
    "    \"Datasets/2022 Ocean Science Funding.csv\",\n",
    "    \"Datasets/2022 Marine Key Biodiversities Areas Percentage.csv\",\n",
    "    \"Datasets/2018 Sustainable Fish Stock.csv\",\n",
    "    \"Datasets/1969 - 2018 capture-fisheries-vs-aquaculture.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file_name in file_names:\n",
    "    df = spark.read.csv(file_name, header=True, inferSchema=True)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore Data\n",
    "for df in dfs:\n",
    "    df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data visualization for each table\n",
    "# 1. 2022 Policy Strength\n",
    "df_0 = dfs[0]\n",
    "df_0 = df_0.withColumn(\"Value\", df_0[\"Value\"].cast(IntegerType()))\n",
    "df_0 = df_0.na.drop()\n",
    "df_0_pd = df_0.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting with Pandas (alternative to Matplotlib for this step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "df_0_pd.plot(kind=\"bar\", x=\"GeoAreaName\", y=\"Value\", rot=90)\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Policy Strength\")\n",
    "plt.title(\"2022 Policy Strength\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. 2022 Ocean Science Funding\n",
    "df_1 = dfs[1]\n",
    "df_1 = df_1.withColumn(\"GDP\", df_1[\"GDP\"].cast(IntegerType()))\n",
    "df_1 = df_1.na.drop()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "df_1_pd = df_1.toPandas()\n",
    "\n",
    "# Plot using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(df_1_pd[\"GeoAreaName\"], df_1_pd[\"GDP\"])\n",
    "plt.xlabel(\"GDP Allocation\")\n",
    "plt.ylabel(\"Country\")\n",
    "plt.title(\"2022 Ocean Science Funding\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3. 2022 Marine Key Biodiversities Areas Percentage (scatter plot)\n",
    "df_2 = dfs[2]\n",
    "df_2 = df_2.withColumn(\"Percent\", df_2[\"MPA Percent\"].cast(IntegerType()))\n",
    "df_2 = df_2.na.drop()\n",
    "df_2_pd = df_2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting with Pandas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_2_pd[\"GeoAreaName\"], df_2_pd[\"Percent\"])\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Percentage\")\n",
    "plt.title(\"2022 Marine Key Biodiversities Areas Percentage\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4. 2018 Sustainable Fish Stock\n",
    "df_3 = dfs[3]\n",
    "df_3 = df_3.withColumn(\"Percentage\", df_3[\"Percentage\"].cast(IntegerType()))\n",
    "df_3 = df_3.na.drop()\n",
    "df_3_pd = df_3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting with Pandas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_3_pd[\"GeoAreaName\"], df_3_pd[\"Percentage\"])\n",
    "plt.xlabel(\"Country\")\n",
    "plt.ylabel(\"Sustainability Rating\")\n",
    "plt.title(\"2018 Sustainable Fish Stock\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5. 1969-2018 Capture Fisheries vs Aquaculture\n",
    "df_4 = dfs[4]\n",
    "df_4 = df_4.na.drop()\n",
    "df_4 = df_4.withColumn(\"Year\", df_4[\"Year\"].cast(IntegerType()))\n",
    "df_4 = df_4.withColumn(\"Capture\", df_4[\"Capture\"].cast(IntegerType()))\n",
    "df_4 = df_4.withColumn(\"Aquaculture\", df_4[\"Aquaculture\"].cast(IntegerType()))\n",
    "df_4 = df_4.na.drop()\n",
    "df_4_pd = df_4.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_4_pd[\"Year\"], df_4_pd[\"Capture\"], color=\"red\", label=\"Capture Fisheries\")\n",
    "plt.plot(df_4_pd[\"Year\"], df_4_pd[\"Aquaculture\"], color=\"blue\", label=\"Aquaculture\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Tonnes\")\n",
    "plt.legend()\n",
    "plt.title(\"Capture Fisheries vs Aquaculture (1969-2018)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Quality report for each dataframe\n",
    "# Quality report for each DataFrame\n",
    "# Quality report for each DataFrame\n",
    "for i, df in enumerate(dfs):\n",
    "    print(f\"Table {i + 1}:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"Missing values:\")\n",
    "    missing_counts = df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "    missing_counts.show()\n",
    "\n",
    "    # Check for duplicate rows\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(df.count() - df.dropDuplicates().count())\n",
    "\n",
    "    # Data types\n",
    "    print(\"Data types:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"Summary statistics:\")\n",
    "    df.describe().show()\n",
    "\n",
    "    print()\n",
    "\n",
    "# Data Preparation\n",
    "# In this section, you can add data preprocessing and transformation steps as needed.\n",
    "# For example, renaming columns, merging dataframes, handling missing values, and feature engineering.\n",
    "\n",
    "# Model Building\n",
    "# You can add machine learning model building and evaluation steps in this section.\n",
    "# For example, building a classification or regression model to address the overfishing problem.\n",
    "\n",
    "# Action Plan and Implementation\n",
    "# Outline your plan for implementing the solution, monitoring, and continuous improvement.\n",
    "\n",
    "# 07-DM\n",
    "# Execute DM task (if applicable)\n",
    "\n",
    "# 08-INT\n",
    "# Summarize Results\n",
    "\n",
    "# Add relevant tables or graphs\n",
    "\n",
    "# 09-ACT\n",
    "# Describe the Action Plan for Implementation, Observation, and Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# 03-DP\n",
    "\n",
    "# Print the current dataframe\n",
    "dfs[3].select(\"GeoAreaName\", \"Percentage\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add SDG14_4_1 column\n",
    "dfs[3] = dfs[3].withColumn(\"SDG14_4_1\", when(dfs[3][\"Percentage\"] > 50, 1).otherwise(0))\n",
    "\n",
    "# Print the updated dataframe\n",
    "dfs[3].select(\"GeoAreaName\", \"Percentage\", \"SDG14_4_1\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# Rename \"Entity\" column to \"GeoAreaName\" in table 5\n",
    "dfs[4] = dfs[4].withColumnRenamed(\"Entity\", \"GeoAreaName\")\n",
    "\n",
    "# Merge the tables based on the \"GeoAreaName\" column\n",
    "merged_df = dfs[0]\n",
    "for i in range(1, len(dfs)):\n",
    "    merged_df = merged_df.join(dfs[i], on=[\"GeoAreaName\"], how=\"outer\")\n",
    "\n",
    "# Display the merged table\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "# Count the number of rows\n",
    "num_rows_left = merged_df.count()\n",
    "\n",
    "# Display the dataframe without missing values and the count\n",
    "print(\"Number of rows left:\", num_rows_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 04-DT: Data Transformation\n",
    "\n",
    "# Remove the \"Code\" column from the merged_df dataframe\n",
    "merged_df = merged_df.drop(\"Code\")\n",
    "\n",
    "# Transform the \"SDG14_4_1\" column values to 1 if true, otherwise 0\n",
    "from pyspark.sql.functions import expr\n",
    "merged_df = merged_df.withColumn(\"SDG14_4_1\", when(expr(\"SDG14_4_1 = true\"), 1).otherwise(0))\n",
    "\n",
    "# Display the modified merged table (first 5 rows)\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter out rows with None values in the \"log_SDG14_4_1\" column\n",
    "filtered_df = merged_df.filter(merged_df[\"SDG14_4_1\"].isNotNull())\n",
    "\n",
    "# Plot a histogram of the log values\n",
    "import matplotlib.pyplot as plt\n",
    "log_values = filtered_df.select(\"SDG14_4_1\").rdd.flatMap(lambda x: x).collect()\n",
    "plt.hist(log_values, bins=[0, 0.5, 1], edgecolor='black')\n",
    "plt.xlabel(\"(SDG14_4_1)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of (SDG14_4_1)\")\n",
    "plt.xticks([0,0.5, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate counts of 0s and 1s in the \"SDG14_4_1\" column\n",
    "counts = merged_df.groupBy(\"SDG14_4_1\").count()\n",
    "counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the modified merged table (first 5 rows)\n",
    "merged_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, when, expr\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a Spark session (if not already created)\n",
    "spark = SparkSession.builder.appName('regression').getOrCreate()\n",
    "\n",
    "# Assuming \"merged_df\" contains the merged DataFrame from the previous steps\n",
    "\n",
    "# 1. Define the features (X) and target (y) for the model\n",
    "feature_cols = [\"GDP\", \"MPA Percent\", \"Aquaculture\", \"Value\"]\n",
    "target_col = \"SDG14_4_1\"\n",
    "\n",
    "# 2. Create a Vector Assembler to assemble feature columns\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "merged_df = assembler.transform(merged_df)\n",
    "\n",
    "# 3. Standardize the features using StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
    "scaler_model = scaler.fit(merged_df)\n",
    "merged_df = scaler_model.transform(merged_df)\n",
    "\n",
    "# 4. Split the data into training and testing sets\n",
    "train_data, test_data = merged_df.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "# 5. Build a regression model using RandomForestRegressor\n",
    "rf = RandomForestRegressor(featuresCol=\"scaled_features\", labelCol=target_col)\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "# 6. Evaluate the model on the test data\n",
    "test_predictions = rf_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol=target_col, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "# 7. Print RMSE (Root Mean Squared Error)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
    "\n",
    "# 8. Create a DataFrame to store results\n",
    "results_df = test_predictions.select(\"GeoAreaName\", target_col, \"prediction\")\n",
    "\n",
    "# 9. Calculate confidence scores based on the predicted probabilities\n",
    "results_df = results_df.withColumn(\"confidence_score\", when(expr(f\"abs({target_col} - prediction) <= {rmse}\"), 1).otherwise(0))\n",
    "\n",
    "# 10. Print the results DataFrame\n",
    "results_df.show()\n",
    "\n",
    "# 11. Calculate and plot the importance of predictors using the feature importances\n",
    "feature_importances = pd.Series(rf_model.featureImportances.toArray(), index=feature_cols)\n",
    "feature_importances.plot(kind='bar')\n",
    "plt.xlabel('Predictors')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# 12. Create a summary table\n",
    "summary_table = pd.DataFrame(columns=[\"Model Information\", \"Evaluation Metrics\"])\n",
    "summary_table.loc[0] = [\"Random Forest Regressor\", f\"RMSE: {rmse}\"]\n",
    "\n",
    "# 13. Add model information and evaluation metrics to the summary table\n",
    "\n",
    "# 14. Calculate the number of correct and wrong predictions\n",
    "correct_predictions = results_df.filter(results_df.confidence_score == 1).count()\n",
    "wrong_predictions = results_df.filter(results_df.confidence_score == 0).count()\n",
    "\n",
    "# 15. Calculate the percentage of correct and wrong predictions\n",
    "total_predictions = results_df.count()\n",
    "percentage_correct = (correct_predictions / total_predictions) * 100\n",
    "percentage_wrong = (wrong_predictions / total_predictions) * 100\n",
    "\n",
    "# 16. Add total, correct, and wrong rows to the summary table\n",
    "summary_table.loc[1] = [\"Total Predictions\", total_predictions]\n",
    "summary_table.loc[2] = [\"Correct Predictions\", correct_predictions]\n",
    "summary_table.loc[3] = [\"Wrong Predictions\", wrong_predictions]\n",
    "summary_table.loc[4] = [\"% Correct\", percentage_correct]\n",
    "summary_table.loc[5] = [\"% Wrong\", percentage_wrong]\n",
    "\n",
    "# 17. Print the updated summary table\n",
    "print(summary_table)\n",
    "\n",
    "# 18. Create a DataFrame to store the predictor importances\n",
    "predictor_importance_df = pd.DataFrame({\n",
    "    \"Predictor\": feature_cols,\n",
    "    \"Importance\": feature_importances.values\n",
    "})\n",
    "\n",
    "# 19. Print the predictor importance table\n",
    "print(predictor_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Preprocess the data\n",
    "pandas_df = merged_df.toPandas()\n",
    "\n",
    "# 2. Define features and target\n",
    "features = [\"GDP\", \"MPA Percent\", \"Aquaculture\", \"Value\"]\n",
    "target = \"SDG14_4_1\"\n",
    "\n",
    "X = pandas_df[features]\n",
    "y = pandas_df[target]\n",
    "\n",
    "# 3. Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 4. Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 5. Build a Regression Neural Network using TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 6. Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=2)\n",
    "\n",
    "# 7. Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "\n",
    "# 8. Calculate predictor importance\n",
    "importance = model.layers[0].get_weights()[0]  # Get weights of the first layer\n",
    "\n",
    "# 9. Create a summary table\n",
    "summary_table = pd.DataFrame(columns=[\"Metric\", \"Value\"])\n",
    "summary_table = summary_table.append({\"Metric\": \"MSE\", \"Value\": mse}, ignore_index=True)\n",
    "\n",
    "# 10. Add model information and evaluation metrics to the summary table\n",
    "# Add more model information or evaluation metrics as needed\n",
    "\n",
    "# 11. Assess model performance\n",
    "# No correct or wrong predictions, but you can calculate additional metrics like RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# 12. Add evaluation metrics to the summary table\n",
    "summary_table = summary_table.append({\"Metric\": \"RMSE\", \"Value\": rmse}, ignore_index=True)\n",
    "\n",
    "# 12. Plot Predictor Importance\n",
    "importance_values = np.abs(importance).mean(axis=1)  # Calculate mean importance across neurons in the first layer\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(features, importance_values)\n",
    "plt.xlabel(\"Predictors\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Predictor Importance\")\n",
    "plt.show()\n",
    "\n",
    "# 13. Create an additional summary table\n",
    "additional_summary_table = pd.DataFrame(columns=[\"Model\", \"Metric\", \"Value\"])\n",
    "additional_summary_table = additional_summary_table.append({\"Model\": \"Regression Neural Network\", \"Metric\": \"Number of Features\", \"Value\": len(features)}, ignore_index=True)\n",
    "additional_summary_table = additional_summary_table.append({\"Model\": \"Regression Neural Network\", \"Metric\": \"Number of Neurons in the First Layer\", \"Value\": 64}, ignore_index=True)\n",
    "additional_summary_table = additional_summary_table.append({\"Model\": \"Regression Neural Network\", \"Metric\": \"Number of Neurons in the Second Layer\", \"Value\": 32}, ignore_index=True)\n",
    "additional_summary_table = additional_summary_table.append({\"Model\": \"Regression Neural Network\", \"Metric\": \"Number of Epochs\", \"Value\": 100}, ignore_index=True)\n",
    "\n",
    "# Print the updated summary table\n",
    "print(\"Updated Summary Table for Regression Neural Network Model:\")\n",
    "print(summary_table)\n",
    "print(\"\\nAdditional Summary Table for Regression Neural Network Model:\")\n",
    "print(additional_summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define features and target\n",
    "features = [\"GDP\", \"MPA Percent\", \"Aquaculture\", \"Value\"]\n",
    "target = \"SDG14_4_1\"\n",
    "\n",
    "# 2. Create a Vector Assembler to combine features into a single vector\n",
    "vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "assembled_df = vector_assembler.transform(merged_df)  # Use the Spark DataFrame directly\n",
    "\n",
    "# 3. Split the data\n",
    "train_data, test_data = assembled_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 4. Build a Decision Tree model\n",
    "dt_model = DecisionTreeRegressor(featuresCol=\"features_vector\", labelCol=target, seed=42)\n",
    "dt_model = dt_model.fit(train_data)\n",
    "\n",
    "# 5. Evaluate the model\n",
    "evaluator = RegressionEvaluator(labelCol=target, predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "predictions = dt_model.transform(test_data)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "# 6. Create a summary table\n",
    "summary_table = pd.DataFrame(columns=[\"Model\", \"Metric\", \"Value\"])\n",
    "summary_table = summary_table.append({\"Model\": \"Decision Tree\", \"Metric\": \"Number of Features\", \"Value\": len(features)}, ignore_index=True)\n",
    "summary_table = summary_table.append({\"Model\": \"Decision Tree\", \"Metric\": \"RMSE\", \"Value\": rmse}, ignore_index=True)\n",
    "\n",
    "# 7. Plot Predictor Importance\n",
    "importances = dt_model.featureImportances.toArray()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(features, importances)\n",
    "plt.xlabel(\"Predictors\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Predictor Importance\")\n",
    "plt.show()\n",
    "\n",
    "# 8. Create the Predictor Importance Table\n",
    "predictor_importance_table = pd.DataFrame({\"Predictor\": features, \"Importance\": importances})\n",
    "\n",
    "# Print the updated summary table and predictor importance\n",
    "print(\"Updated Summary Table for Decision Tree Model:\")\n",
    "print(summary_table)\n",
    "print(\"\\nDecision Tree Predictor Importance Table:\")\n",
    "print(predictor_importance_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done with this tutorial, let's move on to Spark DataFrame Operations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
